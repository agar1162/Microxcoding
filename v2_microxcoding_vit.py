# -*- coding: utf-8 -*-
"""v2 Microxcoding VIT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15d9hZRCdHfEC-DaKS6DjvTgGQ1VExYz8
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/microsoft/microxcaling.git
!pip install torch==2.2.0 torchaudio==2.2.0
!pip install "numpy<2" --upgrade

# %cd microxcaling
!pip install -e .

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/microxcaling
!pip install -e .

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import ViTForImageClassification, ViTImageProcessor
from datasets import load_dataset
from tqdm import tqdm
import time

def make_quant_sim(model, w_elem_format, a_elem_format, block_size=32,
                   scale_bits=8, bfloat=16, custom_cuda=False,
                   quantize_backprop=False, round="nearest"):
    """
    Apply microscaling (MX) quantization to the model using the microxcaling library.

    Args:
        model: The model to quantize
        w_elem_format: Weight element format (e.g., 'fp6_e3m2', 'fp8_e4m3', 'int8')
        a_elem_format: Activation element format (e.g., 'fp6_e3m2', 'fp8_e4m3', 'int8')
        block_size: MX scaling block size (default: 32 for MX-compatible formats)
        scale_bits: Number of bits for the MX shared scale (default: 8)
        bfloat: Bfloat format for vector operations (default: 16 for bfloat16)
        custom_cuda: Use custom CUDA kernels (default: False)
        quantize_backprop: Apply quantization on backward pass (default: False)
        round: Rounding mode ('nearest', 'floor', 'even') (default: 'nearest')
    """
    try:
        from mx import finalize_mx_specs
        from mx import mx_mapping
    except ImportError:
        raise ImportError("microxcaling library not found. Please install it first.")

    # Create MX specs dictionary
    mx_specs = {
        'w_elem_format': w_elem_format,
        'a_elem_format': a_elem_format,
        'block_size': block_size,
        'scale_bits': scale_bits,
        'bfloat': bfloat,
        'custom_cuda': custom_cuda,
        'quantize_backprop': quantize_backprop,
        'round': round,
    }

    # Finalize the specs (this sets defaults and validates)
    mx_specs = finalize_mx_specs(mx_specs)

    if mx_specs is None:
        print("No quantization specified, skipping MX injection")
        return

    print(f"Applying MX quantization with specs: {mx_specs}")

    # Auto-inject MX modules and functions
    # This will replace certain torch.nn.* and torch.nn.functional.*
    # modules/functions in the global namespace!
    mx_mapping.inject_pyt_ops(mx_specs)

    print("MX quantization applied successfully!")

model_name = "google/vit-base-patch16-224"
processor = ViTImageProcessor.from_pretrained(model_name)
model = ViTForImageClassification.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

def load_eval_data(n_samples=500):
    ds = load_dataset("imagenet-1k", split="validation", streaming=True)
    ds = ds.take(n_samples)

    images, labels = [], []
    for sample in tqdm(ds, total=n_samples, desc="Loading dataset"):
        images.append(sample['image'].convert("RGB"))
        labels.append(sample['label'])

    return images, labels

images, labels = load_eval_data(n_samples=2000)

def evaluate(model, images, labels, batch_size=32):
    correct = 0
    n_samples = len(images)

    for i in tqdm(range(0, n_samples, batch_size), desc="Evaluating"):
        batch_images = images[i:i+batch_size]
        batch_labels = labels[i:i+batch_size]

        inputs = processor(images=batch_images, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            predictions = model(**inputs).logits.argmax(-1).tolist()

        correct += sum(p == l for p, l in zip(predictions, batch_labels))

    return round(correct / n_samples * 100, 2)

configs = [
    # Baseline
    ("baseline",           None,           None),

    # Single Precision
    ("fp8_e4m3",           'fp8_e4m3',     'fp8_e4m3'),
    ("fp8_e5m2",           'fp8_e5m2',     'fp8_e5m2'),
    ("fp4_e2m1",           'fp4_e2m1',     'fp4_e2m1'),

    # Mixed Precision
    ("w_fp8_a_fp4",        'fp8_e4m3',     'fp4_e2m1'),
    ("w_fp4_a_fp8",        'fp4_e2m1',     'fp8_e4m3'),
]

def get_quantized_model(w_fmt, a_fmt):
    if w_fmt is not None:
        make_quant_sim(model, w_elem_format=w_fmt, a_elem_format=a_fmt)

    fresh_model = ViTForImageClassification.from_pretrained(model_name).to(device)
    fresh_model.eval()
    return fresh_model

results = []
for label, w_fmt, a_fmt in configs:
    print(f"\nRunning: {label}")

    quantized_model = get_quantized_model(w_fmt, a_fmt)
    accuracy = evaluate(quantized_model, images, labels)
    results.append({"config": label, "accuracy": accuracy})

baseline_acc = results[0]['accuracy']

print("\n\nResults Summary:")
print(f"{'Config':<20} {'Accuracy':>10} {'Acc Drop':>10}")
print("-" * 42)
for r in results:
    drop = r['accuracy'] - baseline_acc
    print(f"{r['config']:<20} {r['accuracy']:>9.2f}% {drop:>+9.2f}%")

fp6_configs = [
    ("baseline",     None,           None),
    ("fp6_e3m2",     'fp6_e3m2',     'fp6_e3m2'),
    ("fp6_e2m3",     'fp6_e2m3',     'fp6_e2m3'),

    # mixed fp6 variants
    ("w_fp6e3_a_fp6e2",  'fp6_e3m2',  'fp6_e2m3'),
    ("w_fp6e2_a_fp6e3",  'fp6_e2m3',  'fp6_e3m2'),
]

fp6_results = []
for label, w_fmt, a_fmt in fp6_configs:
    print(f"\nRunning: {label}")
    quantized_model = get_quantized_model(w_fmt, a_fmt)
    accuracy = evaluate(quantized_model, images, labels)
    fp6_results.append({"config": label, "accuracy": accuracy})

baseline_acc = fp6_results[0]['accuracy']
print("\n\nFP6 Results Summary:")
print(f"{'Config':<25} {'Accuracy':>10} {'Acc Drop':>10}")
print("-" * 47)
for r in fp6_results:
    drop = r['accuracy'] - baseline_acc
    print(f"{r['config']:<25} {r['accuracy']:>9.2f}% {drop:>+9.2f}%")